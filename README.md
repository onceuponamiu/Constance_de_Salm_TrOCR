# ğŸ–‹ï¸ TrOCR-HTR pour la correspondance de Constance de Salm

SystÃ¨me de fine-tuning **TrOCR** (OCR basÃ© sur Transformers) pour la reconnaissance de lâ€™Ã©criture manuscrite dans la correspondance de Constance de Salm.

---

## ğŸ“– Vue dâ€™ensemble

- **TrOCR** est un modÃ¨le *Transformer end-to-end* pour lâ€™OCR :
  - **Vision Transformer (ViT)** â†’ encodeur dâ€™images
  - **GPT-2** â†’ dÃ©codeur de texte
- Ce projet rÃ©alise un fine-tuning de TrOCR sur des donnÃ©es dâ€™Ã©criture manuscrite **historique (XIXe siÃ¨cle)** issues de la correspondance de Constance de Salm.

ğŸ‘‰ DonnÃ©es dâ€™entraÃ®nement uploadÃ©es sur Hugging Face :
ğŸ”— [onceuponamiu/trocr_constance_de_salm_finetune](https://huggingface.co/onceuponamiu/trocr_constance_de_salm_finetune)

---

## ğŸ“Š Exploration des donnÃ©es

### RÃ©sumÃ©
- **Total image-text pairs**: 33
- **Longueur moyenne du texte**: 2360.2 caractÃ¨res
- **Longueur min / max**: 1180 / 3927
- **Textes <10 caractÃ¨res**: 0

### Exemple
- **Sample text**:
  `Chacun connait cet apologue : "Un peintre avait exposÃ© un tableau reprÃ©sentant un lion terrassÃ© "par...`
- **Sample image**:
  `/training-data/verite-terrain/CdS02_Konv002-01_0034.jpg`

### VÃ©rification des dossiers
- âœ… `training-data/verite-terrain/`
  - ğŸ“· Images: 33
  - ğŸ“„ XML: 45
  - ğŸ”— Matching pairs: 33
- âœ… `training-data/sample-images/`
  - ğŸ“· Images: 107
  - ğŸ“„ XML: 0
  - ğŸ”— Matching pairs: 0
- âœ… `training-data/predic-corrigees/`
  - ğŸ“· Images: 0
  - ğŸ“„ XML: 20
  - ğŸ”— Matching pairs: 0

---

## ğŸ“‚ Structure du dÃ©pÃ´t

```
trocr-htr/
â”œâ”€â”€ README.md                 # Ce document
â”œâ”€â”€ requirements.txt          # DÃ©pendances
â”œâ”€â”€ config/                   # Configurations
â”‚   â”œâ”€â”€ base_config.yaml
â”‚   â””â”€â”€ training_config.yaml
â”œâ”€â”€ src/                      # Code source
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ trainer.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ scripts/                  # Scripts principaux
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_Data_Exploration.ipynb
â”‚   â”œâ”€â”€ 02_Training_TrOCR.ipynb
â”‚   â”œâ”€â”€ 03_Evaluation.ipynb
â”‚   â””â”€â”€ 04_Inference_Demo.ipynb
â”œâ”€â”€ data/                     # DonnÃ©es (prÃ©traitÃ©es + train/val/test)
â”œâ”€â”€ edition_numerique/        # Exemples de sorties numÃ©riques
â””â”€â”€ Google_Colab_Setup.ipynb  # Setup rapide pour Colab
```

---

## âš™ï¸ Installation

1.  CrÃ©er un environnement virtuel Python :
    ```bash
    python -m venv trocr_env
    source trocr_env/bin/activate  # Linux/Mac
    trocr_env\Scripts\activate     # Windows
    ```

2.  Installer les dÃ©pendances :
    ```bash
    pip install -r requirements.txt
    ```

## ğŸš€ Utilisation

1.  **PrÃ©parer les donnÃ©es**
    ```bash
    python scripts/prepare_data.py --source_dir training-data/verite-terrain/ --output_dir data/
    ```

2.  **EntraÃ®nement**
    ```bash
    python scripts/train.py --config config/training_config.yaml
    ```

3.  **Ã‰valuation**
    ```bash
    python scripts/evaluate.py --model_path models/final/trocr_cds_model --test_dir data/test/
    ```

4.  **InfÃ©rence locale**
    ```bash
    python scripts/inference.py --model_path models/final/trocr_cds_model --image_path path/to/image.jpg
    ```

### ğŸ““ Notebooks

-   `01_Data_Exploration.ipynb` : Exploration et analyse des donnÃ©es
-   `02_Training_TrOCR.ipynb` : Fine-tuning pas Ã  pas
-   `03_Evaluation.ipynb` : Ã‰valuation dÃ©taillÃ©e
-   `04_Inference_Demo.ipynb` : DÃ©mo dâ€™infÃ©rence sur de nouvelles images

---

## ğŸ”§ Chargement du modÃ¨le fine-tunÃ© depuis Hugging Face

Le modÃ¨le fine-tunÃ© est disponible publiquement :
ğŸ”— **[onceuponamiu/trocr_constance_de_salm_finetune](https://huggingface.co/onceuponamiu/trocr_constance_de_salm_finetune)**

### Exemple en Python
```python
from transformers import VisionEncoderDecoderModel, AutoProcessor
from PIL import Image

# Charger le modÃ¨le et le processor depuis Hugging Face
model_id = "onceuponamiu/trocr_constance_de_salm_finetune"
model = VisionEncoderDecoderModel.from_pretrained(model_id)
processor = AutoProcessor.from_pretrained(model_id)

# Charger une image manuscrite
image = Image.open("path/to/your/image.jpg").convert("RGB")

# PrÃ©parer les inputs
pixel_values = processor(images=image, return_tensors="pt").pixel_values

# GÃ©nÃ©rer la prÃ©diction
generated_ids = model.generate(pixel_values, max_length=512, num_beams=4)
predicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)

print("ğŸ”® Transcription OCR:", predicted_text)
```

**Notes**
- `AutoProcessor` gÃ¨re Ã  la fois preprocessing image (ViT) et tokenisation texte (GPT-2).
- Toujours convertir les images en `RGB`.
- Ajustez `max_length` et `num_beams` pour un meilleur compromis qualitÃ©/temps.

Pour accÃ©lÃ©rer sur GPU :
```python
model.to("cuda")
pixel_values = pixel_values.to("cuda")
```

---

### ğŸ”— IntÃ©gration avec le pipeline
- Peut remplacer ou complÃ©ter les modÃ¨les Kraken.
- Export possible au format ALTO XML compatible TEI.
- Comparaison des performances avec les pipelines existants.

### ğŸ“ˆ Performances attendues
- **TrOCR base** : ~70â€“80% prÃ©cision sur manuscrits historiques
- **TrOCR fine-tunÃ©** : objectif >85% prÃ©cision sur donnÃ©es CdS
- **Vitesse** : ~1â€“2 sec/image (GPU), ~5â€“10 sec/image (CPU)

### ğŸ’» Configuration systÃ¨me
- **RAM** : min. 8 Go (16 Go+ recommandÃ©)
- **GPU** : NVIDIA avec CUDA (fortement recommandÃ©)
- **Stockage** : ~10 Go (modÃ¨le + cache)
- **Python** : 3.8+
